\documentclass[a4paper,12pt]{extarticle}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[
backend=biber,
style=numeric,
maxbibnames=99
]{biblatex}
\addbibresource{refs.bib}
\usepackage[colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false,hypertexnames=true, urlcolor=blue]{hyperref} 
\usepackage{indentfirst}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}
\usepackage{tablefootnote}

\usepackage{chngcntr} % нумерация графиков и таблиц по секциям
\counterwithin{table}{section}
\counterwithin{figure}{section}

\graphicspath{{graphics/}}%путь к рисункам

\makeatletter
% \renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
\makeatother

\geometry{left=2.5cm}% левое поле
\geometry{right=1.0cm}% правое поле
\geometry{top=2.0cm}% верхнее поле
\geometry{bottom=2.0cm}% нижнее поле
\setlength{\parindent}{1.25cm}
\renewcommand{\baselinestretch}{1.5} % междустрочный интервал


\newcommand{\bibref}[3]{\hyperlink{#1}{#2 (#3)}} % biblabel, authors, year
\addto\captionsrussian{\def\refname{Список литературы (или источников)}} 

\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}
\input{title_vkr}% это титульный лист - выберите подходящий вам из имеющихся в проекте вариантов (kr - курсовая работа у 3 курса, vkr - выпускная квалификационная работа у 4 курса)
\newpage
\setcounter{page}{2}

{
	\hypersetup{linkcolor=black}
	\tableofcontents
}

\newpage

\section*{Abstract}

This thesis explores various approaches for multi-label emotion detection in text across multiple languages. We investigate traditional supervised approaches including BERT, SetFit, and Seq2Seq models, as well as a novel retrieval-augmented generation system called EmoRAG. Using the BRIGHTER dataset, which covers 28 languages including many low-resource ones, we demonstrate that our EmoRAG system achieves state-of-the-art performance without requiring extensive model training. This work contributes to the field of multilingual emotion recognition by providing a comparative analysis of different approaches and introducing an efficient, scalable method for detecting emotions across diverse languages.

\addcontentsline{toc}{section}{Abstract}

\section*{Keywords}
Deep learning, emotion detection, multilingual NLP, retrieval-augmented generation, low-resource languages, multi-label classification

\section{Introduction}

Emotions are fundamental to human communication and experience, coloring our interactions, decisions, and perceptions. The ability to detect and understand emotions in text has become increasingly important in natural language processing (NLP), with applications spanning various domains. From customer service and mental health monitoring to social media analysis and human-computer interaction, automated emotion detection systems serve as valuable tools for understanding human sentiment at scale.

\subsection{Context and Motivation}

Over the past decade, emotion detection has emerged as a critical area of research within the broader field of affective computing. Unlike traditional sentiment analysis, which typically focuses on determining whether a text is positive, negative, or neutral, emotion detection aims to identify specific emotional states such as joy, sadness, anger, fear, surprise, and disgust. This fine-grained understanding of affective content enables more nuanced and human-like interactions between computational systems and users.

The applications of emotion detection are diverse and impactful:

\begin{itemize}
\item \textbf{Customer Experience}: Identifying customer emotions in reviews, support tickets, and feedback can help businesses respond appropriately and improve services.
\item \textbf{Mental Health Support}: Detecting emotional distress in text can support early intervention in mental health contexts and assist in monitoring well-being.
\item \textbf{Content Recommendation}: Understanding the emotional impact of content can lead to more personalized recommendations in entertainment and media platforms.
\item \textbf{Social Media Analysis}: Tracking emotional responses to events, products, or public figures provides valuable insights for marketing, policy-making, and crisis management.
\item \textbf{Educational Technology}: Recognizing student emotions can help adaptive learning systems respond to frustration, confusion, or engagement.
\end{itemize}

Despite significant advances in this field, most emotion detection research and applications have focused primarily on high-resource languages, particularly English. This linguistic imbalance creates a significant gap in automated emotion understanding for the majority of the world's population who speak low-resource languages. As digital communication increasingly crosses linguistic boundaries, the need for emotion detection systems that work effectively across multiple languages becomes more pressing.

\subsection{Problem Statement}

The development of effective multi-lingual, multi-label emotion detection systems faces several interconnected challenges:

\textbf{Linguistic Diversity}: Human languages vary dramatically in their lexical, syntactic, and semantic structures. These variations extend to how emotions are expressed, with some cultures having specific emotion concepts that lack direct translations in other languages. For instance, the German "Schadenfreude" (pleasure derived from another's misfortune) or the Portuguese "saudade" (a deep longing for something absent) represent culture-specific emotional concepts. Building systems that can recognize emotions across such diverse linguistic contexts requires approaches that can adapt to these variations.

\textbf{Data Scarcity}: While high-resource languages like English benefit from abundant annotated data, most of the world's languages lack substantial labeled datasets for emotion detection. This data scarcity makes it difficult to train robust models for these languages, particularly when using traditional supervised learning approaches that rely on large amounts of labeled data.

\textbf{Multi-label Complexity}: Emotions often co-occur, with texts frequently expressing multiple emotions simultaneously. This multi-label nature adds complexity to both model architecture and evaluation, requiring methods that can effectively capture relationships between emotions rather than treating them as mutually exclusive categories.

\textbf{Cross-cultural Variations}: The expression and interpretation of emotions vary significantly across cultures, affecting how emotions are verbalized in different languages. These cultural differences present challenges for creating universally applicable emotion detection systems.

\textbf{Computational Efficiency}: Training separate models for each language is computationally expensive and impractical, especially given the thousands of languages spoken worldwide. More efficient approaches are needed to make multi-lingual emotion detection accessible and scalable.

Addressing these challenges requires innovative approaches that can operate effectively with limited labeled data, adapt to linguistic and cultural differences, and handle the inherent complexity of multi-label emotion classification.

\subsection{Research Questions}

This thesis investigates several key research questions that guide our exploration of multi-lingual, multi-label emotion detection:

\begin{enumerate}
\item \textbf{How do traditional supervised approaches compare to retrieval-augmented approaches in emotion detection?}
   We examine whether newer paradigms like retrieval-augmented generation can outperform traditional fine-tuning approaches for emotion detection tasks. This comparison explores the trade-offs between parameter-efficient methods that leverage in-context learning and approaches that require extensive model training.

\item \textbf{How can we effectively handle emotion detection in low-resource languages?}
   We investigate techniques that can transfer knowledge from high-resource to low-resource languages, as well as methods that can perform well with minimal labeled examples. This question addresses the practical challenge of developing emotion detection systems for the majority of the world's languages, which lack extensive training data.

\item \textbf{What are the specific challenges in multi-label emotion detection, and how can they be addressed?}
   We explore the complexities of detecting multiple co-occurring emotions in text and evaluate different strategies for handling these interdependencies. This includes examining how the relationships between emotions can be modeled effectively across different languages and cultural contexts.

\item \textbf{How do different retrieval mechanisms affect the performance of retrieval-augmented generation for emotion detection?}
   We investigate how various retrieval strategies impact the quality of emotion detection, particularly when operating across diverse languages with varying levels of resource availability.

\item \textbf{What is the impact of model ensemble and aggregation strategies on multi-lingual emotion detection performance?}
   We examine how combining predictions from multiple models using different aggregation techniques can improve performance and robustness across languages and emotion categories.
\end{enumerate}

\subsection{Thesis Structure}

The remainder of this thesis is organized as follows:

\textbf{Chapter 2: Related Work} provides a comprehensive review of previous research in emotion detection, multi-label classification, multi-lingual NLP models, retrieval-augmented generation, few-shot learning, and the application of large language models to emotion detection tasks.

\textbf{Chapter 3: Data} describes the BRIGHTER dataset, including its creation, annotation process, composition, and the specific challenges it presents for multi-lingual emotion detection.

\textbf{Chapter 4: Methodology} presents the four approaches we explored: BERT-based fine-tuning, SetFit few-shot learning, Seq2Seq generative models, and our novel EmoRAG system. Each approach is described in detail, including model architecture, training procedures, and implementation specifics.

\textbf{Chapter 5: Experiments} outlines our experimental setup, evaluation metrics, and hyperparameter tuning strategies for each approach.

\textbf{Chapter 6: Results} presents a comprehensive analysis of our experimental findings, including overall performance comparisons, language-specific analyses, emotion-specific evaluations, and ablation studies.

\textbf{Chapter 7: Discussion} interprets our results, comparing the strengths and weaknesses of each approach, analyzing performance on low-resource languages, and examining common error patterns.

\textbf{Chapter 8: Conclusion and Future Work} summarizes our key findings, acknowledges limitations, and suggests promising directions for future research in multi-lingual emotion detection.

The thesis concludes with \textbf{References} and \textbf{Appendices} that provide additional details on prompt templates, hyperparameters, supplementary results, and implementation code.

\section{Related Work}

This chapter reviews the existing literature relevant to multi-lingual, multi-label emotion detection. We examine six key areas: traditional approaches to emotion detection, multi-label classification techniques, multi-lingual NLP models, retrieval-augmented generation systems, few-shot learning in NLP, and the application of large language models to emotion detection tasks.

\subsection{Emotion Detection in Text}

Emotion detection in text has evolved significantly over the past two decades, from lexicon-based approaches to sophisticated neural models. Early work by \cite{strapparava2007semeval} introduced one of the first datasets for emotion recognition in English text, defining the task as detecting Ekman's six basic emotions: joy, sadness, anger, fear, surprise, and disgust. This work established the foundation for most subsequent research in the field and remains influential in how emotions are categorized in computational approaches.

Lexicon-based methods were among the first approaches to emotion detection. \cite{mohammad2013crowdsourcing} developed the NRC Emotion Lexicon, mapping English words to eight emotions and two sentiments. Subsequent work by \cite{thelwall2010sentiment} with SentiStrength provided methods for detecting emotion intensity, showing that strength of emotion can be accurately detected from text. While lexicon-based methods offer interpretability and language-specific insights, they struggle with contextual understanding and require extensive manual annotation for each language.

Supervised machine learning techniques dominated the next wave of emotion detection research. Work by \cite{balahur2013sentiment} demonstrated the effectiveness of Support Vector Machines (SVMs) with careful feature engineering for emotion classification. In their comprehensive evaluation, they showed that n-gram features with term frequency-inverse document frequency (TF-IDF) weighting could accurately distinguish between emotion categories. Similarly, \cite{colneric2018emotion} applied ensemble methods across multiple datasets, highlighting the importance of combining different classification strategies for robust emotion detection.

The deep learning era brought significant advances to emotion detection. \cite{felbo2017using} introduced DeepMoji, which leveraged distant supervision from emojis to pre-train models for emotion recognition. Their approach demonstrated the power of large-scale pre-training on naturally occurring emotional signals. Building on this idea, \cite{abdul2017emonet} developed large-scale Twitter-specific models for emotion detection, showing that domain-specific pre-training significantly improves performance.

More recently, transformer-based approaches have set new benchmarks in emotion detection. \cite{demszky2020goemotions} created GoEmotions, a large-scale dataset with 27 emotion categories, and demonstrated strong performance with BERT-based models. Similarly, \cite{zhang2022emotion} showed how different pre-trained language models compare for emotion detection, with RoBERTa consistently outperforming other architectures. These works highlight the effectiveness of transfer learning from large pre-trained models for understanding emotional content in text.

Despite this progress, most emotion detection research has focused primarily on English and other high-resource languages. The lack of annotated data in diverse languages has limited progress toward truly multi-lingual emotion detection systems. As noted by \cite{plaza2020overview} in their survey of emotion detection in Spanish, techniques that work well for English often need significant adaptation for other languages, particularly those with different morphological structures or cultural contexts for emotion expression.

\subsection{Multi-label Classification}

Multi-label classification, where instances can belong to multiple categories simultaneously, presents unique challenges compared to multi-class classification. Emotion detection is inherently multi-label, as texts often express multiple emotions at once. \cite{tsoumakas2007multi} provided the foundational taxonomy of multi-label classification approaches, dividing them into problem transformation methods and algorithm adaptation methods. This categorization remains useful for understanding the landscape of multi-label techniques.

In problem transformation approaches, \cite{read2011classifier} introduced classifier chains, which model inter-label dependencies by building a sequence of binary classifiers. Their work demonstrated significant improvements over binary relevance methods that treat each label independently. Similarly, \cite{zhang2007ml} proposed the multi-label k-nearest neighbor (ML-kNN) algorithm, adapting the traditional kNN approach to multi-label contexts with statistical information from neighbor labels.

Deep learning approaches have further advanced multi-label classification. \cite{nam2014large} showed that using cross-entropy loss with a sigmoid activation function per label outperforms ranking-based loss functions in neural networks for multi-label classification. Building on this, \cite{chen2019multi} introduced a novel attention mechanism for multi-label text classification that captures both document-level and label-specific representations. Their approach demonstrated state-of-the-art performance on several benchmark datasets.

The challenge of label imbalance in multi-label settings is particularly relevant to emotion detection, as certain emotions appear more frequently than others. \cite{wu2020distribution} addressed this issue with a distribution-balanced loss function that effectively handles both head and tail labels in large-scale multi-label classification. Their work provides valuable insights for emotion detection tasks where some emotions are naturally rarer than others.

Another key challenge in multi-label classification is capturing label correlations. \cite{yeh2017learning} proposed a label-specific attention model that learns different feature representations for different labels while also modeling label correlations. This approach is particularly relevant for emotion detection, where emotions like fear and surprise often co-occur, while others like joy and sadness rarely appear together.

The evaluation of multi-label classification presents its own challenges. \cite{sorower2010literature} provided a comprehensive analysis of evaluation metrics for multi-label classification, demonstrating that different metrics capture different aspects of performance. In emotion detection, this is particularly important as system performance may vary significantly depending on whether one prioritizes precision, recall, or overall F1 score across emotions.

\subsection{Multi-lingual NLP Models}

Advances in multi-lingual NLP models have been crucial for developing systems that work across diverse languages. \cite{devlin2019bert} introduced multilingual BERT (mBERT), pre-trained on 104 languages, demonstrating surprisingly strong cross-lingual transfer abilities despite having no explicit cross-lingual objectives during pre-training. This work established the viability of single models serving multiple languages simultaneously.

Building on these foundations, \cite{conneau2020unsupervised} developed XLM-RoBERTa, a cross-lingual model trained on 100 languages with 2.5 times more data than mBERT. Their extensive evaluations showed that scaling up multilingual pre-training significantly improves performance on downstream tasks across languages. Critically, they demonstrated that the "curse of multilinguality" (degradation in per-language performance as more languages are added) can be mitigated through increased model capacity and training data.

For extremely low-resource languages, \cite{pfeiffer2020mad} introduced Adapter-based architectures that allow parameter-efficient fine-tuning for individual languages. Their MAD-X (Massively Multilingual Adapter-based Transfer) framework enables transfer to languages not seen during pre-training, which is particularly valuable for emotion detection in diverse linguistic contexts.

Beyond model architecture, the creation of multilingual evaluation resources has been crucial for advancing the field. \cite{hu2020xtreme} introduced XTREME, a multi-lingual benchmark covering 40 languages and 9 tasks, providing a standardized way to evaluate multilingual models. Similarly, \cite{ruder2021xtreme} developed XTREME-R, an extension that addresses limitations in the original benchmark and includes more challenging language understanding tasks.

Despite these advances, significant challenges remain in multilingual NLP. \cite{artetxe2020cross} conducted a critical evaluation of zero-shot cross-lingual transfer, showing that current approaches struggle with languages that are linguistically distant from high-resource ones. This limitation is particularly relevant for emotion detection, where cultural and linguistic differences significantly impact how emotions are expressed.

Research by \cite{ponti2019modeling} on cross-lingual transfer for low-resource languages highlighted the importance of typological features in determining transfer effectiveness. They demonstrated that performance drops substantially when target languages have different morphological or syntactic structures from source languages, suggesting the need for language-specific adaptations in emotion detection systems.

\cite{wu2020all} analyzed mBERT's performance across languages, showing a clear correlation between pre-training data size and downstream task performance. Their work underscores the continuing challenges for truly low-resource languages, where even state-of-the-art multilingual models may offer limited benefits without language-specific fine-tuning.

\subsection{Retrieval-Augmented Generation (RAG)}

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing language model capabilities with external knowledge. \cite{lewis2020retrieval} introduced the original RAG framework, combining dense retrieval with sequence-to-sequence models to generate outputs conditioned on relevant retrieved documents. Their approach demonstrated significant improvements over standard language models on knowledge-intensive tasks.

While RAG was initially developed for generation tasks, its application to classification has shown promising results. \cite{mallen2023retrieval} adapted RAG for text classification, demonstrating that retrieved examples can serve as few-shot demonstrations to guide in-context learning. Their approach achieved competitive performance on various classification benchmarks without requiring task-specific fine-tuning.

For multi-lingual contexts, \cite{shi2023replug} developed Cross-Lingual RAG, which leverages a shared dense retrieval space across languages to retrieve relevant documents for low-resource languages. This approach is particularly valuable for emotion detection across diverse languages, as it enables knowledge transfer from high-resource to low-resource languages.

Recent innovations in RAG architectures have further improved performance. \cite{izacard2022atlas} introduced Atlas, a retrieval-augmented model that jointly learns to retrieve and generate, showing superior performance on knowledge-intensive tasks across 26 languages. Their work demonstrates the potential for end-to-end training of retrieval and generation components for multilingual applications.

The selection of retrieved documents significantly impacts RAG performance. \cite{gao2023retrieval} introduced techniques for dynamic retrieval that adapt to the specific needs of each query, showing that adaptive retrieval strategies outperform static approaches. Similarly, \cite{yu2023improving} developed methods for filtering and re-ranking retrieved documents to ensure relevance and diversity, which is particularly important for capturing the range of emotional expressions across languages.

In classification contexts, \cite{singh2022flare} demonstrated that RAG can mitigate the challenge of domain shift by retrieving examples similar to the test instance, regardless of their source domain. This capability is especially valuable for emotion detection, where emotional expressions may vary significantly across domains (e.g., social media vs. literary text) and languages.

Despite these advances, challenges remain in applying RAG to multi-label classification. \cite{glass2022re} highlighted the difficulty of retrieving documents that cover all relevant labels for multi-label tasks, suggesting the need for specialized retrieval strategies that consider label co-occurrence patterns. Their work provides valuable insights for adapting RAG to multi-label emotion detection.

\subsection{Few-shot Learning in NLP}

Few-shot learning has become increasingly important for addressing data scarcity in NLP, particularly for low-resource languages. \cite{brown2020language} demonstrated the remarkable few-shot capabilities of large language models through in-context learning, where a model makes predictions based on a few examples provided in the prompt. This paradigm shift has significant implications for multi-lingual emotion detection, potentially reducing the need for extensive labeled data in each language.

Building on this foundation, \cite{gao2021making} introduced the concept of "making pre-trained language models better few-shot learners" through prompt engineering and demonstration selection. Their work showed that carefully constructed prompts with strategically selected examples can dramatically improve few-shot performance on various NLP tasks, including classification.

For more specialized applications, \cite{tunstall2022efficient} developed SetFit, a few-shot learning approach that combines contrastive learning with classification fine-tuning. By leveraging sentence transformers and efficient pair-wise training, SetFit achieves strong performance with as few as 8 examples per class, making it particularly suitable for low-resource scenarios in emotion detection.

The selection of examples for few-shot learning significantly impacts performance. \cite{liu2022few} showed that retrieving examples based on semantic similarity to the test instance outperforms random selection, especially for complex tasks. Their work introduced a retrieval-based few-shot learning framework that adaptively selects the most relevant examples for each test instance, which is particularly valuable for capturing the contextual nature of emotions.

Cross-lingual few-shot learning presents additional challenges. \cite{zhao2021multi} explored cross-lingual transfer in few-shot settings, demonstrating that examples from high-resource languages can effectively guide predictions in low-resource languages when properly aligned. Their approach leverages multilingual representations to bridge the gap between languages, offering insights for emotion detection in diverse linguistic contexts.

Recent work has also explored the role of meta-learning in few-shot scenarios. \cite{wang2021meta} introduced a meta-learning framework for few-shot text classification that learns to adapt to new tasks with minimal examples. Their approach demonstrates significant improvements over traditional fine-tuning, particularly when the number of available examples is extremely limited.

Despite these advances, few-shot learning in multi-label settings remains challenging. \cite{hou2022few} addressed this gap with a specialized few-shot learning approach for multi-label classification that captures label correlations even with limited examples. Their work offers valuable techniques for emotion detection, where understanding the relationships between emotions is crucial for accurate multi-label prediction.

\subsection{Large Language Models in Emotion Detection}

The emergence of large language models (LLMs) has transformed approaches to emotion detection. \cite{alhussain2021deep} provided a comprehensive survey of deep learning approaches for emotion detection, highlighting the shift from traditional feature-based methods to end-to-end neural approaches. Their review identified transformer-based models as the most promising direction for future research, foreshadowing the current dominance of LLMs.

The in-context learning capabilities of LLMs have proven particularly valuable for emotion detection. \cite{wei2022chain} demonstrated through their "chain-of-thought" prompting approach that LLMs can reason about complex tasks when properly guided. Applied to emotion detection, this technique allows models to articulate the reasoning behind emotional classifications, potentially improving accuracy for subtle or mixed emotional expressions.

For cross-lingual applications, \cite{winata2021language} showed that large multilingual language models can effectively transfer emotion detection capabilities across languages through careful prompt design. Their work demonstrated that providing examples in the target language significantly improves performance, even when the model's pre-training data for that language is limited.

Recent work by \cite{zhang2023emotion} explored the potential of LLMs for zero-shot emotion detection, finding that models like GPT-4 can identify emotions without task-specific examples when given clear instructions. However, they also noted significant performance gaps across emotions and languages, highlighting the continuing challenges for universal emotion detection systems.

The relationship between model size and emotion detection performance has been the subject of several studies. \cite{zhao2023survey} conducted a systematic evaluation of different-sized language models on emotion detection tasks, finding that while larger models generally perform better, the relationship is not linear, and proper prompt engineering can sometimes allow smaller models to match the performance of their larger counterparts.

Beyond accuracy, recent research has focused on the interpretability of emotion detection systems. \cite{park2022social} introduced techniques for extracting explanations from LLMs regarding their emotion classifications, enhancing trust and providing insights into how these models understand emotional content. Their work demonstrates that LLMs can not only classify emotions but also articulate the textual cues that indicate specific emotional states.

Despite these advances, challenges remain in applying LLMs to emotion detection across diverse languages and cultures. \cite{derczynski2022cultural} highlighted how cultural differences in emotion expression can lead to systematic biases in LLM-based emotion detection. Their work emphasizes the need for culturally-aware approaches that can adapt to different emotional expression patterns across languages and contexts.

\subsection{Research Gaps and Opportunities}

Our review of the literature reveals several important gaps that this thesis aims to address:

\begin{enumerate}
    \item \textbf{Limited Multi-lingual Coverage}: While significant progress has been made in multi-lingual NLP, emotion detection research has primarily focused on high-resource languages. Few studies have systematically evaluated approaches across a diverse set of languages spanning multiple language families.

    \item \textbf{Lack of Comparative Analyses}: Most studies focus on a single approach (e.g., fine-tuning BERT or using LLMs), with limited comparative analyses of different paradigms on the same datasets. This makes it difficult to determine which approaches are most effective under different constraints.

    \item \textbf{Retrieval-Augmented Classification}: While RAG has shown promise for generation tasks, its application to multi-label classification, particularly emotion detection, remains underexplored. The potential of combining retrieval with large language models for classification deserves further investigation.

    \item \textbf{Low-Resource Adaptability}: Few studies have explicitly addressed how emotion detection approaches can be adapted for low-resource languages with minimal labeled data. This gap is particularly significant given the thousands of languages with limited NLP resources.

    \item \textbf{Multi-label Emotion Detection}: Most emotion detection research treats emotions as independent categories, with limited exploration of approaches specifically designed to capture the complex co-occurrence patterns of multiple emotions.
\end{enumerate}

\section{Data}

\subsection{Data Splits}

We utilized the official BRIGHTER dataset splits for our experiments:

\begin{itemize}
\item \textbf{Training Set}: $\sim$70\% of the data, used for model training
\item \textbf{Validation Set}: $\sim$15\% of the data, used for hyperparameter tuning and early stopping
\item \textbf{Test Set}: $\sim$15\% of the data, used for final evaluation
\item \textbf{Development Set}: A special subset combining portions of training and validation sets, used for few-shot example selection
\end{itemize}

For low-resource languages, we ensured careful stratification to maintain representative distributions of emotions in all splits.

\subsection{Data Challenges and Considerations}

Working with the BRIGHTER dataset presented several challenges that informed our approach:

\subsubsection{Linguistic Diversity}

The extreme linguistic diversity of BRIGHTER requires models that can generalize across typologically distinct languages. This diversity includes differences in:
\begin{itemize}
\item Morphological complexity (from isolating to highly synthetic languages)
\item Syntactic structure (varying word orders)
\item Script systems (Latin, Cyrillic, Arabic, Devanagari, etc.)
\item Lexical resources for expressing emotions
\end{itemize}

\subsubsection{Data Imbalance}

The dataset exhibits multiple dimensions of imbalance:
\begin{itemize}
\item Varying amounts of data per language
\item Uneven distribution of emotion labels
\item Differences in the prevalence of multi-label instances
\item Varying text lengths and sources across languages
\end{itemize}

\subsubsection{Cultural Differences in Emotion Expression}

Emotion expression varies significantly across cultures, affecting how emotions are verbalized in different languages. These differences include:
\begin{itemize}
\item Culture-specific emotion concepts without direct translations
\item Varying tendencies toward emotional explicitness or implicitness
\item Different metaphorical expressions for emotions
\item Culturally specific emotional associations
\end{itemize}

\subsubsection{Annotation Consistency}

Despite careful guidelines, some variation in annotation practices across languages is inevitable. This includes:
\begin{itemize}
\item Differences in threshold for assigning certain emotion labels
\item Variation in intensity rating calibration
\item Cultural differences in emotion perception affecting annotation
\end{itemize}

These challenges informed our decision to explore both traditional supervised approaches and more flexible retrieval-augmented methods that might better adapt to the diverse nature of the dataset.

\section{Methodology}

This chapter presents the various approaches we explored for multi-lingual, multi-label emotion detection using the BRIGHTER dataset. We investigate both traditional supervised learning approaches (BERT, SetFit, and Seq2Seq) and a novel retrieval-augmented generation (RAG) approach called EmoRAG. Each approach represents a different paradigm in machine learning for text classification:

\begin{enumerate}
\item \textbf{BERT-based Approach}: Fine-tune a pre-trained language model with a classification head for multi-label prediction
\item \textbf{SetFit Approach}: A few-shot learning technique that combines contrastive learning with standard classification techniques
\item \textbf{Seq2Seq Approach}: Framing emotion detection as a text generation task
\item \textbf{EmoRAG System}: A novel retrieval-augmented generation system that leverages few-shot learning without parameter updates
\end{enumerate}

Figure 4.1 provides an overview of the methodological approaches explored in this thesis.

[Figure 4.1: Overview of methodological approaches for multi-lingual emotion detection. Create a diagram showing the four approaches side by side with their main components.]

The following sections describe each approach in detail, including model architecture, training procedures, and implementation specifics.

\subsection{BERT-based Approach}

\begin{lstlisting}[language=Python]
# Model initialization
model = AutoModelForSequenceClassification.from_pretrained(
    config.checkpoint_path, 
    num_labels=len(datasets_labels),
    problem_type="multi_label_classification"
)

# Training arguments
training_args = TrainingArguments(
    output_dir=output_dir,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    save_strategy="epoch",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    weight_decay=0.01,
    num_train_epochs=10
)
\end{lstlisting}

For prediction, we apply a sigmoid function to the model outputs and use a threshold of 0.5 to convert probabilities to binary predictions:

\begin{lstlisting}[language=Python]
probabilities = torch.sigmoid(torch.tensor(predictions)).numpy()
binary_predictions = probabilities > 0.5
\end{lstlisting}

Performance is evaluated using F1-micro and F1-macro metrics, which are particularly suitable for multi-label classification with imbalanced classes.

The BERT-based approach provides a strong baseline for emotion detection, leveraging the powerful contextual representations of transformer models. However, it requires substantial computational resources for training and may struggle with low-resource languages where pre-training data is limited.

\subsection{SetFit Approach}

SetFit (Sentence Transformer Fine-tuning) is a novel few-shot learning method introduced by Tunstall et al. (2022) that combines contrastive learning with standard classification techniques. It was designed to achieve strong performance with limited labeled data, making it particularly suitable for multi-lingual emotion detection where labeled examples may be scarce for low-resource languages.

\subsubsection{SetFit Overview}

The SetFit approach consists of two main stages:

\begin{enumerate}
\item \textbf{Contrastive Learning Stage}: Fine-tune a sentence transformer model using contrastive learning on sentence pairs derived from labeled examples.
\item \textbf{Classification Stage}: Train a classifier (typically a linear model) on the embeddings produced by the fine-tuned sentence transformer.
\end{enumerate}

Figure 4.3 illustrates the SetFit training process.

[Figure 4.3: SetFit training process showing the two-stage approach. The diagram should include the contrastive learning phase with sentence pairs and the classification head training phase. Reference: https://github.com/huggingface/setfit/raw/main/assets/diagram.png]

The key innovation of SetFit is its ability to leverage the power of sentence transformers for few-shot learning without requiring extensive labeled data or computationally expensive prompt-based approaches. By fine-tuning the sentence embeddings directly on the task-specific data, SetFit can adapt pre-trained embeddings to better represent the nuances of emotion detection across languages.

\subsubsection{Multi-target Strategies}

Adapting SetFit to multi-label classification presents unique challenges, as the original method was designed for single-label classification. We explored three strategies for handling multiple emotion labels:

\begin{enumerate}
\item \textbf{Multi-output Strategy}: Train a single classifier that outputs multiple binary predictions, one for each emotion label. This approach treats each emotion independently but allows the model to learn correlations between them.

\item \textbf{Classifier Chain Strategy}: Train multiple classifiers in a chain, where each classifier takes as input both the sentence embedding and the predictions of previous classifiers in the chain. This approach explicitly models dependencies between emotions.

\item \textbf{One-vs-Rest Strategy}: Train separate binary classifiers for each emotion label. This approach is the simplest but ignores potential correlations between emotions.
\end{enumerate}

Figure 4.4 visualizes these three strategies for multi-label classification with SetFit.

\section{Experiments}

\subsection{Experimental Setup}
\begin{itemize}
\item \textbf{Hardware and Software}: Details on the implementation environment
\item \textbf{Model Configurations}: Specific parameters for each approach
\item \textbf{Training Process}: Time, resources, and optimization techniques
\item \textbf{Cross-validation}: Strategies for ensuring robust evaluation
\end{itemize}

\subsection{Evaluation Metrics}
\begin{itemize}
\item \textbf{F1-micro and F1-macro}: Definition and justification
\item \textbf{Language-specific Evaluation}: How performance is assessed across languages
\item \textbf{Emotion-specific Metrics}: Evaluation for each emotion category
\end{itemize}

\subsection{Hyperparameter Tuning}
\begin{itemize}
\item \textbf{BERT Hyperparameters}: Learning rates, batch sizes, etc.
\item \textbf{SetFit Hyperparameters}: Sampling strategies, example counts, etc.
\item \textbf{Seq2Seq Hyperparameters}: Beam width, repetition penalty, etc.
\item \textbf{EmoRAG Hyperparameters}: Example counts, retrieval mechanisms, etc.
\end{itemize}

\section{Results}

\subsection{Overall Performance}
\begin{itemize}
\item \textbf{Comparative Analysis}: Performance of all approaches
\item \textbf{Statistical Significance}: Analysis of performance differences
\item \textbf{Efficiency Comparison}: Training time, inference time, and resource requirements
\end{itemize}

\subsection{Performance by Language}
\begin{itemize}
\item \textbf{High-resource vs. Low-resource Languages}: Analysis of performance differences
\item \textbf{Language Family Analysis}: Patterns across related languages
\item \textbf{Cross-lingual Transfer}: Evidence of knowledge transfer between languages
\end{itemize}

\subsection{Performance by Emotion}
\begin{itemize}
\item \textbf{Emotion-specific Analysis}: Differences in detection accuracy by emotion
\item \textbf{Co-occurrence Patterns}: Analysis of multi-label predictions
\item \textbf{Confusion Analysis}: Common misclassifications between emotions
\end{itemize}

\subsection{Ablation Studies}
\begin{itemize}
\item \textbf{EmoRAG Components}: Impact of different retrievers, generators, and aggregation strategies
\item \textbf{Example Count Analysis}: Performance vs. number of examples
\item \textbf{Cross-model Analysis}: Contribution of different LLMs to the ensemble
\end{itemize}

\section{Discussion}

\subsection{Comparison of Approaches}
\begin{itemize}
\item \textbf{Strengths and Weaknesses}: Analysis of each approach
\item \textbf{Resource Efficiency}: Trade-offs between performance and computational requirements
\item \textbf{Practical Considerations}: Deployment scenarios for different approaches
\end{itemize}

\subsection{Performance on Low-resource Languages}
\begin{itemize}
\item \textbf{Challenges}: Specific issues in low-resource settings
\item \textbf{Transfer Learning Effects}: How well knowledge transfers to low-resource languages
\item \textbf{Retrieval Benefits}: How retrieval mechanisms help in low-resource scenarios
\end{itemize}

\subsection{Error Analysis}
\begin{itemize}
\item \textbf{Common Error Patterns}: Analysis of misclassifications
\item \textbf{Language-specific Challenges}: Linguistic factors affecting performance
\item \textbf{Cultural Factors}: Impact of cultural differences on emotion expression and detection
\end{itemize}

\section{Conclusion and Future Work}
\begin{itemize}
\item \textbf{Summary of Findings}: Key takeaways from the research
\item \textbf{Limitations}: Constraints and shortcomings of the current approaches
\item \textbf{Future Research Directions}: Promising avenues for further investigation
\item \textbf{Broader Impacts}: Potential applications and implications of the work
\end{itemize}

\printbibliography

\section*{Appendices}
\begin{itemize}
\item \textbf{A. Prompt Templates}: Complete prompts used for LLMs
\item \textbf{B. Hyperparameter Details}: Comprehensive listing of all parameters
\item \textbf{C. Additional Results}: Supplementary tables and figures
\item \textbf{D. Implementation Code}: Links to code repositories
\end{itemize}

\end{document}
